# T-LAFS (时间序列语言模型增强特征搜索) 项目文档

**版本**: 1.1
**日期**: 2025年6月15日

---

## 1. 项目动机：从"奥卡姆剃刀"悖论到"特征质量至上"

### 1.1. 初始观察：简单之优
在我们的时间序列预测项目中，一个反复出现的反常现象引起了我们的注意：**在某些任务中，结构简单的模型（如基础的多层感知机 SimpleNN）其预测性能竟稳定地超越了理论上更强大、更复杂的模型（如 Transformer 或 LSTM+Attention）。**

这种现象似乎违背了技术演进的直觉，我们称之为时序预测领域的"奥卡姆剃刀悖论"——更简单的模型反而取得了更好的效果。

### 1.2. 假设与验证：一次关键的认知迭代
我们最初的假设是**"模型-特征不匹配"**。我们推断，当引入一套"大而全"的人工特征集后，其中部分特征（如高度相关的滚动统计量）可能成为"噪声"，对内部结构更敏感的复杂模型（Transformer/LSTM）造成了"毒害"，导致其性能下降，而简单模型则因其结构"忽略"了这些噪声，反而表现更好。

为了验证这一点，我们专门设计了`verify_mismatch_hypothesis.py`脚本进行"取证"。**然而，这次严格的验证实验带来了一个出乎意料但更具启发性的发现：**

| 模型 | R² (仅lag-1特征) | R² (大而全特征集) | 性能变化 |
| :--- | :--- | :--- | :--- |
| SimpleNN | 0.6543 | 0.6962 | **+0.0419 (显著提升)** |
| EnhancedNN | 0.6662 | 0.7147 | **+0.0485 (显著提升)** |
| Transformer | 0.5602 | 0.6658 | **+0.1056 (显著提升)** |

实验结果清晰地表明，一个**精心构建的、信息丰富的"大而全"特征集，能够普适性地提升所有模型的性能**，包括我们预想中会被"毒害"的复杂模型。

### 1.3. 新的认知：特征的"质"远重于"量"
这一结果并没有推翻我们的项目，反而让我们对问题的理解更加深刻。它揭示了：
*   **问题的关键，不在于特征的"数量"**。单纯的"特征过载"可能并非导致性能下降的元凶。
*   **真正的决定因素，在于特征的"质量"与"组合方式"**。我们最初观察到的性能下降现象，极可能是因为当时的人工特征集中包含了某些"低质量"或"不相关"的特征组合，它们引入的噪声和共线性恰好对复杂模型产生了干扰。

这引出了我们的核心论点：**通往高性能模型的路径，并非是简单地增加或减少特征，而是在一个潜在有用的、巨大的特征空间中，进行一场智能化的"寻宝游戏"，以发现那个与特定模型最为匹配的、由高质量特征构成的"黄金子集"。**

手动进行这场"寻宝"无异于大海捞针。因此，一个能够自动化、智能化完成此任务的框架，其必要性与价值便凸显无疑。这就是 **T-LAFS** 诞生的最终原因。

---

## 2. T-LAFS 框架：架构与核心模块原理

T-LAFS 被设计成一个受强化学习（RL）启发的闭环系统。它主要由四大核心模块组成：**策略家（Strategist）**、**执行引擎（Engine）**、**裁判（Judge）** 和 **调度器（Orchestrator）**。

![T-LAFS Architecture](https://mermaid.ink/svg/pako:eNqVVttuwyAQ_BXLVwXauQca6gqpVFVp6qGPOjAOxjaxyImTdP37riNLqmqfEjv7zM7ODLKzK5B2KkO-UCMv2iNq1Tpy7bXp9LgK5oU35hS8E6rF-wD4M8G-jQyD2-oM0_h_qTfT4tHwR9300H4oIIM9_r7J5sM5q5jT9EAFU6iQcE59V_wzGqM9I8R4mJvVn876qC3B4L36gQ8e5J9YQ6p3L0e9_qHjDylsQ-iJm5b-fD3hDazpP4-6h_8fN2W39cT1tP22-8-hLh9H-92w9G5eX3eQe-G66_x-t04Q3R2Qe5L56Hk0Yq7B-4Yq_047tW07V-u8i5j_iF1Tz0B2f4J0IExTjYQGgZ1y4Ue6hTDAp7pL1Y2GgCjD1B0g8j3T-A1F5I7A7xO8F2QkYjR0W80517sM-9jL0DqgI-sI5dKq_kX861K4J591Qe97c_4l5i84qO_Y0q-4U1WqE2M7XbQv4X2aK_xL09-m_N-F_T06Gz3XmJ-1-U_fU6G7B7H0iK-S8m4NfL8XzQ5g8W9f-YkHq5F_K_J9b4iF937vV63XkHhM-E_w-5eIuQ)

### 2.1. 策略家 (The Strategist): 大语言模型 (LLM)

*   **角色**：作为框架的"大脑"，负责创造性地生成特征工程方案。
*   **工作原理**：
    1.  **接收状态 (State)**：在每次迭代中，LLM会接收到一个描述当前系统状态的详细上下文，包括：当前的特征列表、上一轮的探针评估分数（R²）、历史最佳分数、特征数量等。
    2.  **借鉴经验 (In-context Learning)**：为了避免LLM进行无目的的随机探索，我们引入了`ExperienceReplayBuffer`（经验回放池）。该模块会存储过去"成功"（被采纳且带来高回报）和"失败"（被拒绝）的特征方案。在生成新计划前，LLM会回顾这些案例，学习什么样的组合是有效的。
    3.  **生成计划 (Action)**：基于当前状态和历史经验，LLM会生成一个由1-2个步骤组成的简短特征工程计划。该计划以严格的JSON格式输出，例如：`{"plan": [{"operation": "create_fourier_features", "period": 365.25, "order": 2, "id": "F_Year"}]}`。
    4.  **提示工程 (Prompt Engineering)**：我们通过精心设计的系统提示（System Prompt）来约束和引导LLM的行为，明确指出：
        *   优先使用外生特征（如时间、日历）和学习型特征（Embeddings）。
        *   谨慎使用基于目标变量本身衍生的特征（如滞后、滚动），以促使模型学习真正的底层规律，而非简单的自相关性。
        *   禁止在已生成的衍生特征上进行"套娃式"创作，避免引入过多噪声。

### 2.2. 执行引擎 (The Engine): 高级特征生成 (`execute_plan`)

*   **角色**：作为框架的"双手"，精确无误地执行LLM的计划。
*   **工作原理**：这是一个静态核心方法，包含了所有特征的生成逻辑，确保了特征创造过程的稳定和无数据泄漏。
    *   **基础特征库**：支持丰富的特征操作，如`lag`（滞后）、`diff`（差分）、`rolling_mean/std`（滚动统计）、`ewm`（指数加权移动平均）、`time_features`（时间特征提取）、`fourier_features`（傅里叶季节性特征）和`interaction`（特征交叉）等。
    *   **核心创新：基于遮罩自编码器 (MAE) 的多尺度学习型特征**
        
        *   **动机：超越僵化的手动特征**
            传统的特征工程，如"过去7天的滚动平均值"，是人类预设的、僵化的规则。我们希望有一种方法，能让模型**自动地、智能地**从一段时序数据中学习其内部蕴含的复杂动态模式，并将其压缩成一个高信息量的特征。

        *   **方法：遮罩自编码器 (Masked Autoencoder, MAE) 的"完形填空"游戏**
            为了实现上述目标，我们引入了在自我监督学习领域非常强大的MAE范式。您可以将其理解为一个让机器自己陪自己玩的"完形填空"游戏：
            1.  **准备教材 (Input)**：我们从时间序列中取出一个窗口的数据（例如90天）。
            2.  **挖洞 (Masking)**：我们随机地将这段数据中的某些部分"涂黑"或"遮住"（在技术上是将其数值置为0）。
            3.  **游戏目标 (Objective)**：模型的任务，是看着这段"残缺不全"的数据，尝试**完整地复原出原始的、未被遮挡的全部数据**。

            ![MAE Illustration](https://i.imgur.com/4z9jG1c.png)
            *（图片释义：输入一个带有多处遮罩的残缺序列，模型需输出一个完整的重构序列）*

        *   **原理：从"游戏"到"理解"**
            要赢得这个游戏，模型不能靠简单的死记硬背。它必须学会**理解**数据背后的规律。比如，要补全一段温度数据，它需要理解什么是季节性趋势、什么是日夜温差、异常值通常如何表现等等。这种"迫使"模型去理解数据内在结构的训练方式，就是**自我监督学习**。
            
            在技术上，这个"玩家"由两部分组成：
            *   **编码器 (Encoder)**：负责观察残缺的数据，并将其压缩成一个包含"核心理解"的浓缩摘要，我们称之为**"学习型嵌入 (Learned Embedding)"**。
            *   **解码器 (Decoder)**：负责读取这个浓缩摘要，并尝试将"理解"还原为完整的原始数据。

        *   **收获：强大的特征生成器**
            当这个"游戏"训练结束后（即模型已经能很好地重构数据），我们便获得了我们真正想要的东西：一个训练有素的**编码器（Encoder）**。我们扔掉仅在训练中起辅助作用的解码器。
            这个编码器现在是一个强大的特征生成器。当我们的**策略家(LLM)**下达一个`create_learned_embedding`指令时，**执行引擎**就会调用这个预训练好的编码器，对数据集进行滑动窗口处理。每滑过一个窗口，编码器就会输出一个高信息密度的嵌入向量（Embedding）。这个向量就是我们最终得到的、强大的**学习型特征**，它作为新的列加入到我们的数据集中，供下游模型使用。

        *   **多尺度 (Multi-Scale)**：为了捕捉不同时间尺度下的规律，我们分别在多个窗口尺寸（如30天、90天、365天）上独立训练了不同的编码器，让LLM可以根据需要，选择性地生成捕捉月度、季度或年度动态的特征。

### 2.3. 裁判 (The Judge): R²下游任务探针 (`probe_feature_set`)

*   **角色**：作为框架的"标尺"，负责评估LLM提出的新特征集的"真实预测能力"。
*   **工作原理**：
    *   **摒弃代理指标**：我们摒弃了简单的模型重要性或不纯度等代理指标，因为它们与最终预测性能并非强相关。
    *   **模拟真实预测**：该探针直接模拟最终的下游预测任务。它使用生成的特征集，在一个独立的、小型的`DynamicProbeModel`（一个轻量级GRU模型）上进行训练，目标是预测**真实的未来目标值**。
    *   **R²作为核心分数**：探针的最终输出是在验证集上的**R²分数**。这个分数直接反映了特征集对于预测任务的有效性，分数越高，说明特征集越有价值。
    *   **可靠的评估**：整个过程在严格的时序交叉验证下进行，确保了评估的公正性和可靠性。这个分数是驱动整个框架迭代优化的核心信号。

### 2.4. 调度器 (The Orchestrator): T-LAFS 主循环 (`run` method)

*   **角色**：作为框架的"指挥官"，调度各大模块，完成整个自动化搜索过程。
*   **工作原理**：这是一个受强化学习启发的循环流程：
    1.  **初始化**：系统从一个最基础的特征（如`lag1`）开始，并计算一个基线分数。
    2.  **迭代循环开始**：
        a. **获取状态**：收集当前特征集、分数等信息。
        b. **LLM决策**：将状态和历史经验打包成Prompt，发送给**策略家(LLM)**，获取新的特征计划（Action）。
        c. **执行与评估**：**执行引擎**执行该计划，生成新的特征集；然后**裁判(Probe)**对新特征集进行评估，得到新的R²分数。
        d. **决策与更新**：
            *   计算**奖励 (Reward)**：`new_score - current_score`。
            *   将本次尝试（状态、动作、奖励、是否采纳）存入经验池。
            *   如果新分数优于历史最佳分数（或在一定容忍度范围内），则**采纳**新计划，更新当前状态。否则，**拒绝**该计划，保留上一轮的最佳状态。
    3.  **循环结束**：在完成预设的迭代次数后，循环结束。
    4.  **最终验证**：将搜索到的最佳特征集，在一个包含多种模型的**最终验证套件 (`evaluate_on_multiple_models`)**上进行全面测试，找出性能最强的"特征集-模型"组合，并输出最终的性能报告。

---

## 3. 已完成工作与当前成果

基于上述框架，我们使用`min_daily_temps`数据集进行了一轮完整的"再评估"实验。

*   **执行模式**：再评估模式 (Re-evaluation Mode)。
*   **加载的计划**：我们加载了之前一次成功搜索运行中发现的最佳计划。
*   **发现的最佳特征计划**：
    ```json
    [
      { "operation": "create_lag", "feature": "temp", "days": 1, "id": "lag1" },
      { "operation": "create_fourier_features", "period": 365.25, "order": 2, "id": "F_Year" },
      { "operation": "create_learned_embedding", "window": 30, "id": "LE_Monthly" }
    ]
    ```
    这个计划简洁而强大，组合了**短期自相关性 (`lag1`)**、**长期年周期性 (`F_Year`)** 以及通过我们的自编码器学到的**中期月度动态模式 (`LE_Monthly`)**。

*   **最终验证结果**：
    *   **最佳模型**：`EnhancedNN` (一个带有Attention机制的LSTM模型)。
    *   **最佳性能**：**R² = 0.7257**
    *   **对比**：这个结果显著优于两个基准实验（"零特征"和"大而全"人工特征），证明了T-LAFS框架的有效性。

**结论**：T-LAFS框架成功地利用其"探针"发现了一个高质量的特征集。随后，一个专业的时序模型（`EnhancedNN`）能够充分利用这个"量身定制"的特征集，最终取得了高性能的预测结果。这完整地验证了我们从**"特征质量与组合是关键"**这一核心洞察出发的整个研究思路。

---

## 4. 后续步骤与未来展望

1.  **调试与优化自编码器**：当前自编码器的重构能力还有提升空间。下一步，我们将单独调试和完善**Masked Autoencoder**模块，确保它能更精确地捕捉序列中的细节和波动，待其性能满意后，再重新集成回T-LAFS框架。
2.  **扩展特征操作库**：向`execute_plan`中加入更多高级的特征工程技术，例如基于小波变换的特征、更复杂的统计特征等，为LLM提供更广阔的创作空间。
3.  **探索多目标探针**：当前的探针主要关注预测性能（R²）。未来可以探索集成更多维度的探针，例如评估特征的**"稳定性"**（在不同数据子集上性能的方差）和**"简洁性"**（倾向于使用更少的特征），引导LLM找到一个在性能、稳定性和成本之间达到平衡的特征集。
4.  **应用于更多数据集**：将T-LAFS框架推广到更多、更复杂的时间序列数据集上，检验其泛化能力和鲁棒性。

--- 